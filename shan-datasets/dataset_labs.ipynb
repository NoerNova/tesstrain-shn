{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "dataset_repo = \"NorHsangPha/shan-news-taifreedom_com\"\n",
    "\n",
    "dataset = load_dataset(dataset_repo, split=\"train\")\n",
    "contents = dataset[\"content\"]\n",
    "\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text):\n",
    "    sentences = re.split('(?<=\\\\။)', text)\n",
    "    sentences = [s.strip().lstrip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "with open(\"taifreedom.txt\", 'w', encoding='utf-8') as f:\n",
    "    for content in contents:\n",
    "        sentences = split_text(content)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence + '\\n')\n",
    "\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"Saved to taifreedom.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from shannlp import word_tokenize, shan_characters, shan_digits\n",
    "\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "        \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "def remove_latin_text(text):\n",
    "    text = re.sub(r\"[^\\u1000-\\u109f0-9\\s/\\-\\\"']\", '', text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def remove_myanmar_text(text):\n",
    "    allowed_chars = set(shan_characters + shan_digits + \"/-'\\\"\")\n",
    "    tokens = word_tokenize(text, engine=\"newmm\")\n",
    "    cleaned_words = []\n",
    "    \n",
    "    for word in tokens:\n",
    "        is_shan_word = True\n",
    "        for char in word:\n",
    "            if char not in allowed_chars and not char.isspace() and not char.isnumeric():\n",
    "                is_shan_word = False\n",
    "                break\n",
    "        if is_shan_word:\n",
    "            cleaned_words.append(word)\n",
    "    \n",
    "    cleaned_text = \"\".join(cleaned_words)\n",
    "    \n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def clean_shan_text(text, keep_numbers=True):\n",
    "    \n",
    "    text = remove_emojis(text)\n",
    "\n",
    "    text = text.replace(\"၊\", \"၊ \").replace(\"။\", \"။ \").replace(\" ၊\", \"၊ \").replace(\" ။\", \"။ \").strip()\n",
    "    text = re.sub(r\"ႉ{2,}\", \"ႉ\", text)\n",
    "    text = text.replace(\"ႆၢ\", \"ၢႆ\")\n",
    "    text = text.replace(\"ေတ\", \"တေ\")\n",
    "\n",
    "    text = remove_latin_text(text)\n",
    "    text = remove_myanmar_text(text)\n",
    "    \n",
    "    # Latin Numbers\n",
    "    numbers = r\"0-9\" if keep_numbers else \"\"\n",
    "    text = re.sub(rf'[^{numbers}{shan_characters}\\s]', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shannlp import word_tokenize\n",
    "\n",
    "text = \"\"\"ဝၢၼ်ႈယေႇပူႇၵေႃႉၸႅပ်ႉ ဝၼ်းထီႉ 17/2/2017 \"ဝၢၼ်ႈ apple ၼမ်ႉၽူး\" apple hello world. !! ရေပူကကော့စပ်ရွာ 4-5\"\"\"\n",
    "\n",
    "print(word_tokenize(text, engine=\"newmm\"))\n",
    "print(remove_latin_text(text))\n",
    "print(remove_myanmar_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pathlib\n",
    "import subprocess\n",
    "\n",
    "training_text_file = './shannews.txt'\n",
    "\n",
    "lines = []\n",
    "\n",
    "with open(training_text_file, 'r') as input_file:\n",
    "    for line in input_file.readlines():\n",
    "        lines.append(line.strip())\n",
    "\n",
    "output_directory = \"../data/shn-ground-truth\"\n",
    "fonts_dir = '/home/noernova/Labs/tesstrain/shan-datasets/fonts'\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.mkdir(output_directory)\n",
    "\n",
    "random.shuffle(lines)\n",
    "\n",
    "count = 250000\n",
    "lines = lines[:count]\n",
    "\n",
    "line_count = 0\n",
    "\n",
    "def get_font_name(line_count, total_count):\n",
    "    fonts = [\"GreatHorKham Taunggyi\", \"Myanmar Text\", \"PangLong Italic\", \"Pyidaungsu\", \"Shan\"]\n",
    "    num_fonts = len(fonts)\n",
    "    \n",
    "    range_size = total_count // num_fonts\n",
    "    \n",
    "    font_ranges = [(i * range_size, (i + 1) * range_size, fonts[i]) for i in range(num_fonts)]\n",
    "    \n",
    "    for min_val, max_val, font in font_ranges:\n",
    "        if min_val <= line_count < max_val:\n",
    "            return font\n",
    "    \n",
    "    return \"Shan\"  # Default\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    line = clean_shan_text(line)\n",
    "\n",
    "    # remove short sentences\n",
    "    if len(line) < 20:\n",
    "        continue\n",
    "\n",
    "    fonts_name = get_font_name(line_count, total_count=count)\n",
    "\n",
    "    training_text_file_name = pathlib.Path(training_text_file).stem\n",
    "    line_training_text = os.path.join(output_directory, f'{training_text_file_name}_{line_count}.gt.txt')\n",
    "    with open(line_training_text, 'w') as output_file:\n",
    "        output_file.writelines([line])\n",
    "\n",
    "    file_base_name = f'{training_text_file_name}_{line_count}'\n",
    "\n",
    "    subprocess.run([\n",
    "        'text2image',\n",
    "        f'--font={fonts_name}',\n",
    "        f'--fonts_dir={fonts_dir}',\n",
    "        f'--text={line_training_text}',\n",
    "        f'--outputbase={output_directory}/{file_base_name}',\n",
    "        '--max_pages=1',\n",
    "        '--strip_unrenderable_words',\n",
    "        '--leading=32',\n",
    "        '--xsize=3600',\n",
    "        '--ysize=480',\n",
    "        '--char_spacing=1.0',\n",
    "        '--exposure=0',\n",
    "        '--unicharset_file=data/shn/unicharset'\n",
    "    ])\n",
    "\n",
    "    line_count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesseract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
