{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import time\n",
    "from OCRDataGenerator import OCRDataGenerator\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import unicodedata\n",
    "from shannlp import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def measure_time(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Record start time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Execute the function\n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        # Record end time\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate and print execution time\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Function '{func.__name__}' took {execution_time:.4f} seconds to execute\")\n",
    "        \n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Docx text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_docx_text(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    all_text = []\n",
    "    buffer_text = \"\"\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        words = para.text.split(\" \")\n",
    "        \n",
    "        for word in words:\n",
    "            if len(buffer_text) + len(word) + 1 > 30:\n",
    "                all_text.append(buffer_text.strip())\n",
    "                buffer_text = word\n",
    "            else:\n",
    "                buffer_text += \" \" + word\n",
    "    \n",
    "    if buffer_text:\n",
    "        all_text.append(buffer_text.strip())\n",
    "        \n",
    "    return all_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images_from_docx(docx_path, fonts, output_dir):\n",
    "    generator = OCRDataGenerator(font_paths=fonts)\n",
    "\n",
    "    texts = extract_docx_text(docx_path)\n",
    "    \n",
    "    for text in texts:\n",
    "        text = text.strip()\n",
    "        \n",
    "        if len(text) < 1:\n",
    "            continue\n",
    "\n",
    "        image, metadata = generator.generate_image(\n",
    "            text=text,\n",
    "            min_font_size=24,\n",
    "            max_font_size=48,\n",
    "            horizontal_padding=40,\n",
    "            vertical_padding=20,\n",
    "            min_height=64,\n",
    "            add_noise=False,\n",
    "            random_transform=False\n",
    "        )\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Image size: {metadata['image_size']}\\n\")\n",
    "\n",
    "        ts = time.time()\n",
    "\n",
    "        # save TIF\n",
    "        image.save(f\"{output_dir}/{ts}.tif\")\n",
    "            \n",
    "        # save TXT\n",
    "        with open(f\"{output_dir}/{ts}.gt.txt\", \"w\", encoding='utf-8') as text_file:\n",
    "            text_file.write(text)\n",
    "\n",
    "        print(f\"Saved image for word: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "datasets = Path(\"./kawtai-dataset\")\n",
    "output_dir = \"./output\"\n",
    "fonts = [\n",
    "    \"./Shan.ttf\",\n",
    "    \"./PangLong.ttf\"\n",
    "]\n",
    "\n",
    "for file_path in datasets.rglob(\"*.docx\"):\n",
    "    if file_path.is_file():\n",
    "        docx_path = file_path\n",
    "        generate_images_from_docx(docx_path, fonts, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "        \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "def clean_latin_text(text):\n",
    "    text = re.sub(r\"[^\\u1000-\\u109f\\s]\", '', text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_shan_text(text, keep_numbers=False):\n",
    "    text = text.replace(\"၊\", \"၊ \").replace(\"။\", \"။ \").replace(\" ၊\", \"၊ \").replace(\" ။\", \"။ \").strip()\n",
    "    text = re.sub(r\"ႉ{2,}\", \"ႉ\", text)\n",
    "    text = text.replace(\"ႆၢ\", \"ၢႆ\")\n",
    "    text = text.replace(\"ေတ\", \"တေ\")\n",
    "\n",
    "    text = clean_latin_text(text)\n",
    "    \n",
    "    # Core Shan-specific characters (characters that are uniquely Shan)\n",
    "    shan_specific_chars = (\n",
    "        # Shan consonants that are unique to Shan\n",
    "        \"\\u1075\\u1076\\u1077\\u1078\\u1079\\u107A\\u107B\\u107C\\u107D\\u107E\\u107F\\u1080\\u1081\"\n",
    "        # Shan vowels and tones that are unique to Shan\n",
    "        \"\\u1082\\u1083\\u1084\\u1085\\u1086\\u1087\\u1088\\u1089\\u108A\"\n",
    "    )\n",
    "    \n",
    "    # Additional characters allowed in Shan words\n",
    "    shan_shared_chars = (\n",
    "        \"\\u1004\\u101E\\u1010\\u1011\\u1015\\u1019\\u101A\\u101B\\u101C\\u101D\\u1022\"  # Shared consonants\n",
    "        \"\\u102D\\u102E\\u102F\\u1030\\u1031\\u1035\\u103A\\u103B\\u103C\\u103D\\u1038\\u1062\"  # Shared vowels and medials\n",
    "    )\n",
    "    \n",
    "    # Numbers and punctuation\n",
    "    shan_numbers = r\"[႐-႙0-9]\" if keep_numbers else r\"[႐-႙]\"  # Shan numbers ႐-႙\n",
    "    shan_punctuations = \"\\u104a\\u104b\\ua9e6\"  # ။၊ꧦ\n",
    "    \n",
    "    # Normalize Unicode\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    \n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "    cleaned_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        word = remove_emojis(word)\n",
    "        \n",
    "        # If the word contains Shan-specific chars or Shan numbers, process it\n",
    "        if (any(char in word for char in shan_specific_chars) or\n",
    "            any(char in word for char in shan_numbers) or\n",
    "            re.search(shan_numbers, word)):\n",
    "            \n",
    "            # Create pattern of all allowed characters\n",
    "            allowed_chars = f\"{shan_specific_chars}{shan_shared_chars}{shan_numbers}{shan_punctuations}\"\n",
    "            pattern = rf\"[^{allowed_chars}\\s]\"\n",
    "            \n",
    "            # Clean the word\n",
    "            cleaned_word = re.sub(pattern, \"\", word)\n",
    "            if cleaned_word:  # Only add non-empty words\n",
    "                cleaned_words.append(cleaned_word)\n",
    "    \n",
    "    return \" \".join(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: ငဝ်, Clean token: \n",
      "Token: ယိုဝ်ႈယိုတ်း, Clean token: ယိုဝ်ႈယိုတ်း\n",
      "Token: ယူႇ, Clean token: ယူႇ\n"
     ]
    }
   ],
   "source": [
    "text = \"ငဝ်ယိုဝ်ႈယိုတ်းယူႇ\"\n",
    "text2 = \"ဢူးၺီႇၼတ်ႉဦးညီနပ်\"\n",
    "\n",
    "text_token = word_tokenize(text, engine=\"newmm\")\n",
    "\n",
    "for token in text_token:\n",
    "    print(f\"Token: {token}, Clean token: {clean_shan_text(token)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toknizing words...\n",
      "Chunking text...\n",
      "ဢူး\n",
      "ၺီႇ\n",
      "ၼတ်ႉ\n",
      "ဦးညီနပ်\n",
      " \n",
      "ဢၼ်\n",
      "ယၢမ်ႈ\n",
      "ယၢဝ်း\n",
      "ၵ\n",
      "ႂ\n",
      "ု\n",
      "ႇ\n",
      "လႆႈ\n",
      "ၶၢဝ်း\n",
      " \n",
      "123\n",
      " \n",
      "ၼၼ်\n",
      " \n",
      "တေ\n",
      "ၶဝ်ႈမႃး\n",
      "ပဵၼ်\n",
      "သုၼ်ႇ\n",
      "ၼိုင်ႈ\n",
      "ၼႂ်း\n",
      "ၸၢတ်ႈ\n",
      " \n",
      "ဢမ်ႇလႆႈ\n",
      "ႁၢမ်း\n",
      "ၵၢတ်ႈ\n",
      "ၽူႈ\n",
      "ႁတ်းႁၢၼ်\n",
      "Final validate chunking text...\n",
      "Function 'split_shan_chunks' took 0.0928 seconds to execute\n",
      "Chunk 1 (length 45): ၺီႇၼတ်ႉဢၼ်ယၢမ်ႈၵႂႇလႆႈၶၢဝ်းၼၼ်ၶဝ်ႈမႃးပဵၼ်သုၼ်ႇ\n",
      "Chunk 2 (length 43): ၼိုင်ႈၼႂ်းၸၢတ်ႈဢမ်ႇလႆႈႁၢမ်းၵၢတ်ႈၽူႈႁတ်းႁၢၼ်\n"
     ]
    }
   ],
   "source": [
    "@measure_time\n",
    "def split_shan_chunks(text, min_len=20, max_len=50):\n",
    "    # Initial text preprocessing\n",
    "    # text = text.replace(\"၊\", \"၊ \").replace(\"။\", \"။ \").replace(\" ၊\", \"၊ \").replace(\" ။\", \"။ \").strip()\n",
    "    print(\"Toknizing words...\")\n",
    "    tokens = word_tokenize(text, engine=\"newmm\")\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    print(\"Chunking text...\")\n",
    "    for token in tokens:\n",
    "        print(token)\n",
    "        token = clean_shan_text(token)\n",
    "        token_len = len(token)\n",
    "        \n",
    "        # If adding token exceeds max_len, finalize current chunk\n",
    "        if current_length + token_len > max_len and current_chunk:\n",
    "            # If current chunk is too short, try to merge with previous\n",
    "            if current_length < min_len and chunks:\n",
    "                last_chunk = chunks.pop()\n",
    "                chunks.append(last_chunk + ' ' + ''.join(current_chunk))\n",
    "            else:\n",
    "                chunks.append(''.join(current_chunk))\n",
    "            current_chunk = [token]\n",
    "            current_length = token_len\n",
    "        else:\n",
    "            # Add token to current chunk\n",
    "            current_chunk.append(token)\n",
    "            current_length += token_len\n",
    "            \n",
    "            # If we hit max_len exactly or have a good split point\n",
    "            if (current_length >= min_len and token in (' ', '။')) or current_length == max_len:\n",
    "                chunks.append(''.join(current_chunk))\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "    \n",
    "    # Handle remaining tokens\n",
    "    if current_chunk:\n",
    "        if current_length < min_len and chunks:\n",
    "            last_chunk = chunks.pop()\n",
    "            chunks.append(last_chunk + ' ' + ''.join(current_chunk))\n",
    "        else:\n",
    "            chunks.append(''.join(current_chunk))\n",
    "    \n",
    "    # Clean up whitespace in chunks\n",
    "    chunks = [re.sub(r'\\s+', ' ', chunk.strip()) for chunk in chunks if chunk.strip()]\n",
    "    \n",
    "    # Final validation pass\n",
    "    print(\"Final validate chunking text...\")\n",
    "    final_chunks = []\n",
    "    i = 0\n",
    "    while i < len(chunks):\n",
    "        current = chunks[i]\n",
    "        \n",
    "        # If chunk exceeds max_len, force split at last valid space\n",
    "        if len(current) > max_len:\n",
    "            split_idx = max_len\n",
    "            while split_idx > 0 and current[split_idx] not in (' ', '။'):\n",
    "                split_idx -= 1\n",
    "            if split_idx > min_len:\n",
    "                final_chunks.append(current[:split_idx].strip())\n",
    "                chunks.insert(i + 1, current[split_idx:].strip())\n",
    "            else:\n",
    "                final_chunks.append(current)\n",
    "        else:\n",
    "            final_chunks.append(current)\n",
    "        i += 1\n",
    "    \n",
    "    return final_chunks\n",
    "\n",
    "# Test the function\n",
    "text = \"ဢူးၺီႇၼတ်ႉဦးညီနပ် ဢၼ်ယၢမ်ႈယၢဝ်းၵႂုႇလႆႈၶၢဝ်း 123 ၼၼ် တေၶဝ်ႈမႃးပဵၼ်သုၼ်ႇၼိုင်ႈၼႂ်းၸၢတ်ႈ ဢမ်ႇလႆႈႁၢမ်းၵၢတ်ႈၽူႈႁတ်းႁၢၼ်\"\n",
    "chunks = split_shan_chunks(text)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1} (length {len(chunk)}): {chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure_time\n",
    "def generate_images_from_huggingface(dataset_repo, chunk_size, fonts, output_dir):\n",
    "    generator = OCRDataGenerator(font_paths=fonts)\n",
    "    chunk_count = 0\n",
    "    \n",
    "    # Load dataset from Hugging Face\n",
    "    dataset = load_dataset(dataset_repo, split=\"train\")\n",
    "    contents = dataset[\"content\"]\n",
    "\n",
    "    print(\"Generate images...\")\n",
    "    for content in contents:\n",
    "        content = content.strip()\n",
    "        content = clean_shan_text(content, keep_numbers=True)\n",
    "\n",
    "        texts = split_shan_chunks(content)\n",
    "\n",
    "        for text in texts:\n",
    "            if len(text) < 1:\n",
    "                continue\n",
    "            \n",
    "            text = re.sub(r\"(^။)|(^၊)\", \"\", text) # remove start ၊, ။\n",
    "            text = text.strip()\n",
    "\n",
    "            image, metadata = generator.generate_image(\n",
    "                text=text,\n",
    "                min_font_size=24,\n",
    "                max_font_size=48,\n",
    "                horizontal_padding=40,\n",
    "                vertical_padding=20,\n",
    "                min_height=64,\n",
    "                add_noise=True,\n",
    "                random_transform=False\n",
    "            )\n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"Image size: {metadata['image_size']}\\n\")\n",
    "\n",
    "            ts = time.time()\n",
    "\n",
    "            # Save TIF\n",
    "            image.save(f\"{output_dir}/{ts}.tif\")\n",
    "\n",
    "            # Save TXT\n",
    "            with open(f\"{output_dir}/{ts}.gt.txt\", \"w\", encoding='utf-8') as text_file:\n",
    "                text_file.write(text)\n",
    "\n",
    "            print(f\"Saved image for word: {text}\")\n",
    "\n",
    "            chunk_count += 1\n",
    "            print(f\"chunk size: {chunk_count}\")\n",
    "\n",
    "            if chunk_count > chunk_size:\n",
    "                return\n",
    "        \n",
    "        print(f\"Total chunk size: {chunk_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../data/shn-ground-truth\"\n",
    "fonts = [\n",
    "    \"Shan.ttf\",\n",
    "    \"PangLong.ttf\",\n",
    "    \"GreatHorKham_Taunggyi.ttf\",\n",
    "    \"mmrtext.ttf\",\n",
    "    \"Pyidaungsu.ttf\"\n",
    "]\n",
    "\n",
    "huggingface_datasets_repo = [\n",
    "    \"NorHsangPha/shan-novel-tainovel_com\",\n",
    "    \"NorHsangPha/shan-news-shannews_org\",\n",
    "    # \"NorHsangPha/shan-news-taifreedom_com\",\n",
    "    # \"NorHsangPha/shan-news-shanhumanrights_org\",\n",
    "    # \"NorHsangPha/shan-news-ssppssa_org\",\n",
    "]\n",
    "\n",
    "for repo in huggingface_datasets_repo:\n",
    "    generate_images_from_huggingface(dataset_repo=repo, chunk_size=200000, fonts=fonts, output_dir=output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesseract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
