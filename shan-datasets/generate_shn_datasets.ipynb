{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import time\n",
    "from OCRDataGenerator import OCRDataGenerator\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import unicodedata\n",
    "from shannlp import word_tokenize, shan_characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def measure_time(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Record start time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Execute the function\n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        # Record end time\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate and print execution time\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Function '{func.__name__}' took {execution_time:.4f} seconds to execute\")\n",
    "        \n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Docx text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_docx_text(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    all_text = []\n",
    "    buffer_text = \"\"\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        words = para.text.split(\" \")\n",
    "        \n",
    "        for word in words:\n",
    "            if len(buffer_text) + len(word) + 1 > 30:\n",
    "                all_text.append(buffer_text.strip())\n",
    "                buffer_text = word\n",
    "            else:\n",
    "                buffer_text += \" \" + word\n",
    "    \n",
    "    if buffer_text:\n",
    "        all_text.append(buffer_text.strip())\n",
    "        \n",
    "    return all_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images_from_docx(docx_path, fonts, output_dir):\n",
    "    generator = OCRDataGenerator(font_paths=fonts)\n",
    "\n",
    "    texts = extract_docx_text(docx_path)\n",
    "    \n",
    "    for text in texts:\n",
    "        text = text.strip()\n",
    "        \n",
    "        if len(text) < 1:\n",
    "            continue\n",
    "\n",
    "        image, metadata = generator.generate_image(\n",
    "            text=text,\n",
    "            min_font_size=24,\n",
    "            max_font_size=48,\n",
    "            horizontal_padding=40,\n",
    "            vertical_padding=20,\n",
    "            min_height=64,\n",
    "            add_noise=False,\n",
    "            random_transform=False\n",
    "        )\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Image size: {metadata['image_size']}\\n\")\n",
    "\n",
    "        ts = time.time()\n",
    "\n",
    "        # save TIF\n",
    "        image.save(f\"{output_dir}/{ts}.tif\")\n",
    "            \n",
    "        # save TXT\n",
    "        with open(f\"{output_dir}/{ts}.gt.txt\", \"w\", encoding='utf-8') as text_file:\n",
    "            text_file.write(text)\n",
    "\n",
    "        print(f\"Saved image for word: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "datasets = Path(\"./kawtai-dataset\")\n",
    "output_dir = \"./output\"\n",
    "fonts = [\n",
    "    \"./Shan.ttf\",\n",
    "    \"./PangLong.ttf\"\n",
    "]\n",
    "\n",
    "for file_path in datasets.rglob(\"*.docx\"):\n",
    "    if file_path.is_file():\n",
    "        docx_path = file_path\n",
    "        generate_images_from_docx(docx_path, fonts, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "        \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "def remove_latin_text(text):\n",
    "    text = re.sub(r\"[^\\u1000-\\u109f\\s]\", '', text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def remove_myanmar_text(text):\n",
    "    tokens = word_tokenize(text, engine=\"newmm\")\n",
    "    cleaned_words = []\n",
    "    for word in tokens:\n",
    "        is_shan_word = True\n",
    "        for char in word:\n",
    "            if char not in shan_characters and not char.isspace():\n",
    "                is_shan_word = False\n",
    "                break\n",
    "        if is_shan_word:\n",
    "            cleaned_words.append(word)\n",
    "    \n",
    "    cleaned_text = \"\".join(cleaned_words)\n",
    "    \n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "@measure_time\n",
    "def clean_shan_text(text, keep_numbers=False):\n",
    "    # Normalize Unicode\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    \n",
    "    text = remove_emojis(text)\n",
    "\n",
    "    text = text.replace(\"၊\", \"၊ \").replace(\"။\", \"။ \").replace(\" ၊\", \"၊ \").replace(\" ။\", \"။ \").strip()\n",
    "    text = re.sub(r\"ႉ{2,}\", \"ႉ\", text)\n",
    "    text = text.replace(\"ႆၢ\", \"ၢႆ\")\n",
    "    text = text.replace(\"ေတ\", \"တေ\")\n",
    "\n",
    "    text = remove_latin_text(text)\n",
    "    text = remove_myanmar_text(text)\n",
    "    \n",
    "    # Latin Numbers\n",
    "    numbers = r\"0-9\" if keep_numbers else \"\"\n",
    "    text = re.sub(rf'[^{numbers}{shan_characters}\\s]', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ငဝ်ယိုဝ်ႈယိုတ်းယူႇ\"\n",
    "text2 = \"ဢူးၺီႇၼတ်ႉ ဦးညီနပ် သျှမ်းပြည်\"\n",
    "text3 = \"နန်းမာလာဝမ်း\"\n",
    "text4 = \"ၸုမ်းၸွႆႈထႅမ် FYO တီႈဝဵင်းလႃႈသဵဝ်ႈၵေႃႈလၢတ်ႈဝႆႉတီႈသိုဝ်ႇၶၢဝ်ႇယႂ်ႇလၢႆလၢႆတီႈဝႃႈ –  လုၵ်ႈဢွၼ်ႇၽၢၼ်ပေႃႈမႄႈတီႈႁူင်းႁဵၼ်းၼၼ်ႉ ယၢမ်းလဵဝ် လူဝ်ႇၶူဝ်းၶွင်ၸႂ်ႉသွႆလႄႈ လွင်ႈၵိၼ်ယမ်ႉ  ။\"\n",
    "text5 =\"ဢူးပွၵ့်ၼီႇလိၼ်း (ဦးပေါက်နီလင်း)\"\n",
    "\n",
    "print(clean_shan_text(text5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure_time\n",
    "def split_shan_chunks(text, min_len=20, max_len=50):\n",
    "    # Initial text preprocessing\n",
    "    text = clean_shan_text(text)\n",
    "    text = text.replace(\"၊\", \"၊ \").replace(\"။\", \"။ \").replace(\" ၊\", \"၊ \").replace(\" ။\", \"။ \").strip()\n",
    "\n",
    "    tokens = word_tokenize(text, engine=\"newmm\")\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        token_len = len(token)\n",
    "        \n",
    "        # If adding token exceeds max_len, finalize current chunk\n",
    "        if current_length + token_len > max_len and current_chunk:\n",
    "            # If current chunk is too short, try to merge with previous\n",
    "            if current_length < min_len and chunks:\n",
    "                last_chunk = chunks.pop()\n",
    "                chunks.append(last_chunk + ' ' + ''.join(current_chunk))\n",
    "            else:\n",
    "                chunks.append(''.join(current_chunk))\n",
    "            current_chunk = [token]\n",
    "            current_length = token_len\n",
    "        else:\n",
    "            # Add token to current chunk\n",
    "            current_chunk.append(token)\n",
    "            current_length += token_len\n",
    "            \n",
    "            # If we hit max_len exactly or have a good split point\n",
    "            if (current_length >= min_len and token in (' ', '။')) or current_length == max_len:\n",
    "                chunks.append(''.join(current_chunk))\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "    \n",
    "    # Handle remaining tokens\n",
    "    if current_chunk:\n",
    "        if current_length < min_len and chunks:\n",
    "            last_chunk = chunks.pop()\n",
    "            chunks.append(last_chunk + ' ' + ''.join(current_chunk))\n",
    "        else:\n",
    "            chunks.append(''.join(current_chunk))\n",
    "    \n",
    "    # Clean up whitespace in chunks\n",
    "    chunks = [re.sub(r'\\s+', ' ', chunk.strip()) for chunk in chunks if chunk.strip()]\n",
    "    \n",
    "    # Final validation pass\n",
    "    final_chunks = []\n",
    "    i = 0\n",
    "    while i < len(chunks):\n",
    "        current = chunks[i]\n",
    "        \n",
    "        # If chunk exceeds max_len, force split at last valid space\n",
    "        if len(current) > max_len:\n",
    "            split_idx = max_len\n",
    "            while split_idx > 0 and current[split_idx] not in (' ', '။'):\n",
    "                split_idx -= 1\n",
    "            if split_idx > min_len:\n",
    "                final_chunks.append(current[:split_idx].strip())\n",
    "                chunks.insert(i + 1, current[split_idx:].strip())\n",
    "            else:\n",
    "                final_chunks.append(current)\n",
    "        else:\n",
    "            final_chunks.append(current)\n",
    "        i += 1\n",
    "    \n",
    "    return final_chunks\n",
    "\n",
    "# Test the function\n",
    "text = \"ၼႂ်းၶေႃႈပိုၼ်ၽၢဝ်ႇၼၼ်ႉ ပႃးဝႆႉထႅင်ႈဝႃႈ ဢူးပွၵ့်ၼီႇလိၼ်း (ဦးပေါက်နီလင်း) ၵႅမ်ၽူႈၵွၼ်းၸႄႈတွၼ်ႈ မိူင်းမႂ်ႇ တေ ၶိုၼ်ႈမႃးပဵၼ် ၽူႈၵွၼ်းၸႄႈတွၼ်ႈႁူဝ်ပၢင်ႇ။ လီႇၵျူင်းယႅၼ် (လီကျုံးရန်) ၵႅမ်ၽူႈၸီႉသင်ႇ ၸုမ်းဢုပ်ႉပိူင်ႇၽွင်းငမ်း ၸႄႈဝဵင်းႁူဝ်ပၢင်ႇ ၸူဝ်ႈၶၢဝ်း တေၶိုၼ်ႈမႃးပဵၼ် ၵႅမ်ၽူႈၸတ်းၵၢၼ် ၸႄႈတွၼ်ႈႁူဝ်ပၢင်ႇ။ ဢူးၺီႇၼတ်ႉ(ဦးညီနပ်) ဢၼ်ယၢမ်ႈႁပ်ႉပုၼ်ႈၽွၼ်း ပဵၼ်ၽူႈၵွၼ်း ၼႂ်းၼႃႈလိၼ် ဢုပ်ႉပိူင်ႇၽွင်းငမ်းဝႃႉ တႂ်ႈၶွင်ႇသီႇသိုၵ်းမၢၼ်ႈ ၼၼ်ႉသမ်ႉ  ၶိုၼ်းႁပ်ႉပုၼ်ႈၽွၼ်း ပဵၼ်ၵႅမ်ၽူႈၵွၼ်း ၸႄႈတွၼ်ႈ  – ဝႃႈၼႆ ။\"\n",
    "chunks = split_shan_chunks(text)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1} (length {len(chunk)}): {chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@measure_time\n",
    "def generate_images_from_huggingface(dataset_repo, chunk_size, fonts, output_dir):\n",
    "    generator = OCRDataGenerator(font_paths=fonts)\n",
    "    chunk_count = 0\n",
    "    \n",
    "    # Load dataset from Hugging Face\n",
    "    dataset = load_dataset(dataset_repo, split=\"train\")\n",
    "    contents = dataset[\"content\"]\n",
    "\n",
    "    print(\"Generate images...\")\n",
    "    for content in contents:\n",
    "        content = content.strip()\n",
    "        content = clean_shan_text(content, keep_numbers=True)\n",
    "\n",
    "        texts = split_shan_chunks(content)\n",
    "\n",
    "        for text in texts:\n",
    "            if len(text) < 1:\n",
    "                continue\n",
    "            \n",
    "            text = re.sub(r\"(^။)|(^၊)\", \"\", text) # remove start ၊, ။\n",
    "            text = text.strip()\n",
    "\n",
    "            image, metadata = generator.generate_image(\n",
    "                text=text,\n",
    "                min_font_size=24,\n",
    "                max_font_size=48,\n",
    "                horizontal_padding=40,\n",
    "                vertical_padding=20,\n",
    "                min_height=64,\n",
    "                add_noise=True,\n",
    "                random_transform=False\n",
    "            )\n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"Image size: {metadata['image_size']}\\n\")\n",
    "\n",
    "            ts = time.time()\n",
    "\n",
    "            # Save TIF\n",
    "            image.save(f\"{output_dir}/{ts}.tif\")\n",
    "\n",
    "            # Save TXT\n",
    "            with open(f\"{output_dir}/{ts}.gt.txt\", \"w\", encoding='utf-8') as text_file:\n",
    "                text_file.write(text)\n",
    "\n",
    "            print(f\"Saved image for word: {text}\")\n",
    "\n",
    "            chunk_count += 1\n",
    "            print(f\"chunk size: {chunk_count}\")\n",
    "\n",
    "            if chunk_count > chunk_size:\n",
    "                return\n",
    "        \n",
    "        print(f\"Total chunk size: {chunk_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../data/shn-ground-truth\"\n",
    "fonts = [\n",
    "    \"fonts/Shan.ttf\",\n",
    "    \"fonts/PangLong.ttf\",\n",
    "    \"fonts/GreatHorKham_Taunggyi.ttf\",\n",
    "    \"fonts/mmrtext.ttf\",\n",
    "    \"fonts/Pyidaungsu.ttf\"\n",
    "]\n",
    "\n",
    "huggingface_datasets_repo = [\n",
    "    # \"NorHsangPha/shan-novel-tainovel_com\",\n",
    "    # \"NorHsangPha/shan-news-shannews_org\",\n",
    "    \"NorHsangPha/shan-news-taifreedom_com\",\n",
    "    \"NorHsangPha/shan-news-shanhumanrights_org\",\n",
    "    \"NorHsangPha/shan-news-ssppssa_org\",\n",
    "]\n",
    "\n",
    "for repo in huggingface_datasets_repo:\n",
    "    generate_images_from_huggingface(dataset_repo=repo, chunk_size=200000, fonts=fonts, output_dir=output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesseract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
