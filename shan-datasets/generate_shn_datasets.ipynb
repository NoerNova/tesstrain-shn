{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import time\n",
    "from OCRDataGenerator import OCRDataGenerator\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "def extract_docx_text(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    all_text = []\n",
    "    buffer_text = \"\"\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        words = para.text.split(\" \")\n",
    "        \n",
    "        for word in words:\n",
    "            if len(buffer_text) + len(word) + 1 > 30:\n",
    "                all_text.append(buffer_text.strip())\n",
    "                buffer_text = word\n",
    "            else:\n",
    "                buffer_text += \" \" + word\n",
    "    \n",
    "    if buffer_text:\n",
    "        all_text.append(buffer_text.strip())\n",
    "        \n",
    "    return all_text\n",
    "\n",
    "def chunk_text(paragraphs):\n",
    "    all_text = []\n",
    "    buffer_text = \"\"\n",
    "\n",
    "    words = paragraphs.split(\" \")\n",
    "    \n",
    "    for word in words:\n",
    "        if len(buffer_text) + len(word) + 1 > 30:\n",
    "            all_text.append(buffer_text.strip())\n",
    "            buffer_text = word\n",
    "        else:\n",
    "            buffer_text += \" \" + word\n",
    "    \n",
    "    if buffer_text:\n",
    "        all_text.append(buffer_text.strip())\n",
    "\n",
    "    return all_text\n",
    "\n",
    "def clean_shan_text(text, keep_numbers=False):\n",
    "    # Unicode range for Shan script: U+1000‚ÄìU+109F\n",
    "    # shan_regex = r\"[\\u1000-\\u109F]+\"\n",
    "    shan_regex = r\"·ÄÄ-·Çü\"\n",
    "    number_regex = r\"\\d\" if keep_numbers else \"\"\n",
    "    \n",
    "    # Combine regex patterns for allowed characters\n",
    "    allowed_chars = f\"{shan_regex}{number_regex}\"\n",
    "    \n",
    "    # Remove emojis and unwanted characters\n",
    "    cleaned_text = re.sub(rf\"[^{allowed_chars}\\s]\", \"\", text)\n",
    "    \n",
    "    return cleaned_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello World ·Äë·Å¢·Äô·Ä∫·ÇÅ·ÇÉ·Äï·Ä±·ÇÉ·Çà·Äô·ÇÑ·Çà·Äô·Åº·Ä∫·Ä∏·Åµ·Ä∞·Çà·Å∂·Ä≠·ÄØ·Åº·Ä∫·Ä∏·ÇÅ·ÄΩ·ÄÑ·Ä∫·Çâ·ÇÅ·ÄΩ·ÄÑ·Ä∫·Çâ·ÇÅ·ÇÜ·Çà·Äê·Ä≠·Åµ·Ä∫·Ä∏·Äê·Ä≠·Åµ·Ä∫·Ä∏·Äö·Äù·Ä∫·Çâüò≠üò≠ 5 ·Äù·Åº·Ä∫·Ä∏ \"\n",
    "\n",
    "print(clean_shan_text(text, keep_numbers=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images_from_docx(docx_path, fonts, output_dir):\n",
    "    generator = OCRDataGenerator(font_paths=fonts)\n",
    "\n",
    "    texts = extract_docx_text(docx_path)\n",
    "    \n",
    "    for text in texts:\n",
    "        text = text.strip()\n",
    "        \n",
    "        if len(text) < 1:\n",
    "            continue\n",
    "\n",
    "        image, metadata = generator.generate_image(\n",
    "            text=text,\n",
    "            min_font_size=24,\n",
    "            max_font_size=48,\n",
    "            horizontal_padding=40,\n",
    "            vertical_padding=20,\n",
    "            min_height=64,\n",
    "            add_noise=False,\n",
    "            random_transform=False\n",
    "        )\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Image size: {metadata['image_size']}\\n\")\n",
    "\n",
    "        ts = time.time()\n",
    "\n",
    "        # save TIF\n",
    "        image.save(f\"{output_dir}/{ts}.tif\")\n",
    "            \n",
    "        # save TXT\n",
    "        with open(f\"{output_dir}/{ts}.gt.txt\", \"w\", encoding='utf-8') as text_file:\n",
    "            text_file.write(text)\n",
    "\n",
    "        print(f\"Saved image for word: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images_from_huggingface(dataset_repo, chunk_size, fonts, output_dir):\n",
    "    generator = OCRDataGenerator(font_paths=fonts)\n",
    "    chunk_count = 0\n",
    "    \n",
    "    # Load dataset from Hugging Face\n",
    "    dataset = load_dataset(dataset_repo, split=\"train\")\n",
    "    contents = dataset[\"content\"]\n",
    "    \n",
    "    for content in contents:\n",
    "        texts = chunk_text(content)\n",
    "\n",
    "        for text in texts:\n",
    "            text = text.strip()\n",
    "            text = clean_shan_text(text, keep_numbers=True)\n",
    "            \n",
    "            if len(text) < 1:\n",
    "                continue\n",
    "\n",
    "            image, metadata = generator.generate_image(\n",
    "                text=text,\n",
    "                min_font_size=24,\n",
    "                max_font_size=48,\n",
    "                horizontal_padding=40,\n",
    "                vertical_padding=20,\n",
    "                min_height=64,\n",
    "                add_noise=False,\n",
    "                random_transform=False\n",
    "            )\n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"Image size: {metadata['image_size']}\\n\")\n",
    "\n",
    "            ts = time.time()\n",
    "\n",
    "            # Save TIF\n",
    "            image.save(f\"{output_dir}/{ts}.tif\")\n",
    "\n",
    "            # Save TXT\n",
    "            with open(f\"{output_dir}/{ts}.gt.txt\", \"w\", encoding='utf-8') as text_file:\n",
    "                text_file.write(text)\n",
    "\n",
    "            print(f\"Saved image for word: {text}\")\n",
    "\n",
    "            chunk_count += 1\n",
    "            print(f\"chunk size: {chunk_count}\")\n",
    "\n",
    "            if chunk_count > chunk_size:\n",
    "                return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "datasets = Path(\"./kawtai-dataset\")\n",
    "output_dir = \"./output\"\n",
    "fonts = [\n",
    "    \"./Shan.ttf\",\n",
    "    \"./PangLong.ttf\"\n",
    "]\n",
    "\n",
    "for file_path in datasets.rglob(\"*.docx\"):\n",
    "    if file_path.is_file():\n",
    "        docx_path = file_path\n",
    "        generate_images_from_docx(docx_path, fonts, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../data/shn-ground-truth\"\n",
    "fonts = [\n",
    "    \"./Shan.ttf\",\n",
    "    \"./PangLong.ttf\",\n",
    "    \"./GreatHorKham_Taunggyi.ttf\"\n",
    "]\n",
    "\n",
    "huggingface_datasets_repo = [\n",
    "    \"NorHsangPha/shan-novel-tainovel_com\",\n",
    "    # \"NorHsangPha/shan-news-shannews_org\",\n",
    "    # \"NorHsangPha/shan-news-taifreedom_com\",\n",
    "    # \"NorHsangPha/shan-news-shanhumanrights_org\",\n",
    "    # \"NorHsangPha/shan-news-ssppssa_org\",\n",
    "]\n",
    "\n",
    "for repo in huggingface_datasets_repo:\n",
    "    generate_images_from_huggingface(dataset_repo=repo, chunk_size=1000000, fonts=fonts, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"NorHsangPha/shan-novel-tainovel_com\", split=\"train\")\n",
    "\n",
    "dataset[\"content\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesseract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
